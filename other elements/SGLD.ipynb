{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SGLD.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyO446SLJRMVUyEXDgzmbRMC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"metadata":{"id":"LdWp48P8ZE1m","executionInfo":{"status":"ok","timestamp":1651366686640,"user_tz":240,"elapsed":15703,"user":{"displayName":"YUNHANG LIN","userId":"04558973430853141896"}},"outputId":"c79e97c0-8b0b-4107-9e8b-d0743e842aeb","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torchvision\n","import torchvision.transforms as transforms\n","from torch.utils.data import random_split\n","import torch.optim as optim \n","import numpy as np\n","import os\n","import math\n","import random\n","from tqdm import tqdm\n","from torch.optim.optimizer import Optimizer, required\n"],"metadata":{"id":"uTQ3ZgmZNdR1","executionInfo":{"status":"ok","timestamp":1651366689107,"user_tz":240,"elapsed":2469,"user":{"displayName":"YUNHANG LIN","userId":"04558973430853141896"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","execution_count":3,"metadata":{"id":"4ICeRyCIHN_n","executionInfo":{"status":"ok","timestamp":1651366689107,"user_tz":240,"elapsed":3,"user":{"displayName":"YUNHANG LIN","userId":"04558973430853141896"}}},"outputs":[],"source":["def load_data(data_set, label_noise, augment_data=False):\n","    \"\"\"\n","    Helper Function to Load data in the form of a tensorflow data set, apply label noise, and return the\n","    train data and test data.\n","\n","    Parameters\n","    ----------\n","    data_set - str, name of data set to load from tf.keras.datasets\n","    label_noise - float, percentage of training data to add noise to\n","    augment_data - boolean, whether or not to use random cropping and horizontal flipping to augment training data\n","    \"\"\"\n","\n","    datasets = [\"cifar10\", \"cifar100\", \"mnist\"]\n","\n","    # load Cifar 10, Cifar 100, or mnis data set\n","    if data_set == \"cifar10\":\n","      transform = transforms.Compose([transforms.ToTensor()])\n","      trainset = torchvision.datasets.CIFAR10(root='/content/gdrive/MyDrive/6699/data', train=True, download=True, transform=transform)\n","      testset = torchvision.datasets.CIFAR10(root='/content/gdrive/MyDrive/6699/data', train=False, download=True, transform=transform)\n","    # elif data_set == \"cifar100\":\n","    #     get_data = tf.keras.datasets.cifar100\n","    # elif data_set == \"mnist\":\n","    #     get_data = tf.keras.datasets.mnist\n","    else:\n","      raise Exception(\n","          f\"Please enter a data set from the following options: {datasets}\"\n","      )\n","    \n","    # apply label noise to the data set\n","    if 0 < label_noise:\n","      random_idx = np.random.choice(\n","          trainset.data.shape[0], int(label_noise * trainset.data.shape[0])\n","      )\n","      rand_labels = np.random.randint(\n","          low=min(trainset.targets), high=max(trainset.targets), size=len(random_idx)\n","      )\n","      # print(trainset.targets[random_idx[0]])\n","      # print(trainset.targets[random_idx[1]])\n","      for i in range(len(random_idx)):\n","\n","        trainset.targets[random_idx[i]] = rand_labels[i]\n","    # print(trainset.targets[random_idx[0]])\n","    # print(trainset.targets[random_idx[1]])\n","    return trainset, testset"]},{"cell_type":"code","source":["class SGLD(Optimizer):\n","    \"\"\" Stochastic Gradient Langevin Dynamics Sampler with preconditioning.\n","        Optimization variable is viewed as a posterior sample under Stochastic\n","        Gradient Langevin Dynamics with noise rescaled in eaach dimension\n","        according to RMSProp.\n","    \"\"\"\n","    def __init__(self,\n","          params,\n","          lr=1e-2,\n","          precondition_decay_rate=0.95,\n","          num_pseudo_batches=1,\n","          num_burn_in_steps=3000,\n","          diagonal_bias=1e-8) -> None:\n","        \"\"\" Set up a SGLD Optimizer.\n","\n","        Parameters\n","        ----------\n","        params : iterable\n","            Parameters serving as optimization variable.\n","        lr : float, optional\n","            Base learning rate for this optimizer.\n","            Must be tuned to the specific function being minimized.\n","            Default: `1e-2`.\n","        precondition_decay_rate : float, optional\n","            Exponential decay rate of the rescaling of the preconditioner (RMSprop).\n","            Should be smaller than but nearly `1` to approximate sampling from the posterior.\n","            Default: `0.95`\n","        num_pseudo_batches : int, optional\n","            Effective number of minibatches in the data set.\n","            Trades off noise and prior with the SGD likelihood term.\n","            Note: Assumes loss is taken as mean over a minibatch.\n","            Otherwise, if the sum was taken, divide this number by the batch size.\n","            Default: `1`.\n","        num_burn_in_steps : int, optional\n","            Number of iterations to collect gradient statistics to update the\n","            preconditioner before starting to draw noisy samples.\n","            Default: `3000`.\n","        diagonal_bias : float, optional\n","            Term added to the diagonal of the preconditioner to prevent it from\n","            degenerating.\n","            Default: `1e-8`.\n","\n","        \"\"\"\n","        if lr < 0.0:\n","            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n","        if num_burn_in_steps < 0:\n","            raise ValueError(\"Invalid num_burn_in_steps: {}\".format(num_burn_in_steps))\n","\n","        defaults = dict(\n","            lr=lr, precondition_decay_rate=precondition_decay_rate,\n","            num_pseudo_batches=num_pseudo_batches,\n","            num_burn_in_steps=num_burn_in_steps,\n","            diagonal_bias=1e-8,\n","        )\n","        super().__init__(params, defaults)\n","\n","\n","    def step(self, closure=None):\n","        loss = None\n","\n","        if closure is not None:\n","            loss = closure()\n","\n","        for group in self.param_groups:\n","            for parameter in group[\"params\"]:\n","\n","                if parameter.grad is None:\n","                    continue\n","\n","                state = self.state[parameter]\n","                lr = group[\"lr\"]\n","                num_pseudo_batches = group[\"num_pseudo_batches\"]\n","                precondition_decay_rate = group[\"precondition_decay_rate\"]\n","                gradient = parameter.grad.data\n","\n","                #  State initialization {{{ #\n","\n","                if len(state) == 0:\n","                    state[\"iteration\"] = 0\n","                    state[\"momentum\"] = torch.ones_like(parameter)\n","\n","                #  }}} State initialization #\n","\n","                state[\"iteration\"] += 1\n","\n","                momentum = state[\"momentum\"]\n","\n","                #  Momentum update {{{ #\n","                momentum.add_(\n","                    (1.0 - precondition_decay_rate) * ((gradient ** 2) - momentum)\n","                )\n","                #  }}} Momentum update #\n","\n","                if state[\"iteration\"] > group[\"num_burn_in_steps\"]:\n","                    sigma = 1. / torch.sqrt(torch.tensor(lr))\n","                else:\n","                    sigma = torch.zeros_like(parameter)\n","\n","                preconditioner = (\n","                    1. / torch.sqrt(momentum + group[\"diagonal_bias\"])\n","                )\n","\n","                scaled_grad = (\n","                    0.5 * preconditioner * gradient * num_pseudo_batches +\n","                    0.01 * torch.normal(\n","                        mean=torch.zeros_like(gradient),\n","                        std=torch.ones_like(gradient)\n","                    ) * sigma * torch.sqrt(preconditioner)\n","                )\n","\n","                parameter.data.add_(-lr * scaled_grad)\n","\n","        return loss"],"metadata":{"id":"xQiZutjfJP_c","executionInfo":{"status":"ok","timestamp":1651366689107,"user_tz":240,"elapsed":3,"user":{"displayName":"YUNHANG LIN","userId":"04558973430853141896"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["def CNN(width):\n","  layers = nn.Sequential(\n","      nn.Conv2d(in_channels=3, out_channels=width, kernel_size=3, stride=1, padding=1),\n","      nn.BatchNorm2d(width),\n","      nn.ReLU(),\n","      # nn.MaxPool2d(kernel_size=2, stride=2),\n","      \n","      nn.Conv2d(in_channels=width, out_channels=2*width, kernel_size=3, stride=1, padding=1),\n","      nn.BatchNorm2d(2*width),\n","      nn.ReLU(),\n","      nn.MaxPool2d(kernel_size=2, stride=2),\n","\n","      nn.Conv2d(in_channels=2*width, out_channels=4*width, kernel_size=3, stride=1, padding=1),\n","      nn.BatchNorm2d(4*width),\n","      nn.ReLU(),\n","      nn.MaxPool2d(kernel_size=2, stride=2),\n","\n","      nn.Conv2d(in_channels=4*width, out_channels=8*width, kernel_size=3, stride=1, padding=1),\n","      nn.BatchNorm2d(8*width),\n","      nn.ReLU(),\n","      nn.MaxPool2d(kernel_size=2, stride=2),\n","\n","      nn.MaxPool2d(kernel_size=4, stride=4),\n","      nn.ReLU(),\n","      nn.Flatten(),\n","      nn.Linear(in_features=8*width, out_features=10),\n","\n","    )\n","\n","  return layers"],"metadata":{"id":"VEezhA9Ul7LI","executionInfo":{"status":"ok","timestamp":1651366689108,"user_tz":240,"elapsed":3,"user":{"displayName":"YUNHANG LIN","userId":"04558973430853141896"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["model = CNN(1)\n","x = torch.rand([5,3,32,32])\n","for name, module in model.named_children():\n","  x = module(x)\n","  # print(name)\n","  print(\"({}) : {}\".format(name,x.shape))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qvHMIvvU-xII","executionInfo":{"status":"ok","timestamp":1651366689386,"user_tz":240,"elapsed":281,"user":{"displayName":"YUNHANG LIN","userId":"04558973430853141896"}},"outputId":"bc085c84-2859-48fd-c109-603365f93c68"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["(0) : torch.Size([5, 1, 32, 32])\n","(1) : torch.Size([5, 1, 32, 32])\n","(2) : torch.Size([5, 1, 32, 32])\n","(3) : torch.Size([5, 2, 32, 32])\n","(4) : torch.Size([5, 2, 32, 32])\n","(5) : torch.Size([5, 2, 32, 32])\n","(6) : torch.Size([5, 2, 16, 16])\n","(7) : torch.Size([5, 4, 16, 16])\n","(8) : torch.Size([5, 4, 16, 16])\n","(9) : torch.Size([5, 4, 16, 16])\n","(10) : torch.Size([5, 4, 8, 8])\n","(11) : torch.Size([5, 8, 8, 8])\n","(12) : torch.Size([5, 8, 8, 8])\n","(13) : torch.Size([5, 8, 8, 8])\n","(14) : torch.Size([5, 8, 4, 4])\n","(15) : torch.Size([5, 8, 1, 1])\n","(16) : torch.Size([5, 8, 1, 1])\n","(17) : torch.Size([5, 8])\n","(18) : torch.Size([5, 10])\n"]}]},{"cell_type":"code","source":["class inverse_squareroot_lr:\n","  \"\"\"\n","  This is the learning rate used with SGD in the paper (Inverse square root decay).\n","  Learning Rate starts at 0.1 and then drops every 512 batches.\n","  \"\"\"\n","\n","  def __init__(self, n_steps=512, init_lr=0.1):\n","      self.n = n_steps\n","      self.gradient_steps = 0\n","      self.init_lr = init_lr\n","\n","  def __call__(self):\n","      lr = self.init_lr / math.sqrt(\n","          1.0 + math.floor(self.gradient_steps / self.n)\n","      )\n","      self.gradient_steps += 1\n","      return lr"],"metadata":{"id":"zUxzvpZSr2XX","executionInfo":{"status":"ok","timestamp":1651366689387,"user_tz":240,"elapsed":3,"user":{"displayName":"YUNHANG LIN","userId":"04558973430853141896"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["class Classifier():\n","\n","    def __init__(self, name, model, trainset, testset, use_cuda=False):\n","        \n","        '''\n","        @name: Experiment name. Will define stored results etc. \n","        @model: Either a GradBasicNet() or a GradAlexNet()\n","        @dataloaders: Dictionary with keys train, val and test and corresponding dataloaders\n","        @class_names: list of classes, where the idx of class name corresponds to the label used for it in the data\n","        @use_cuda: whether or not to use cuda\n","        '''\n","        \n","        self.name = name\n","        if use_cuda and not torch.cuda.is_available():\n","            raise Exception(\"Asked for CUDA but GPU not found\")\n","            \n","        self.use_cuda = use_cuda\n","        \n","        self.model = model.to('cuda' if use_cuda else 'cpu')\n","        self.trainset = trainset\n","        self.testset = testset\n","        self.init_lr = 0.1\n","        self.criterion = nn.CrossEntropyLoss() #use cross entropy loss\n","        self.optim = SGLD(model.parameters(), lr=self.init_lr) #use SGD with suggest hyperparams; you must select all the model params\n","        # self.optim = optim.SGD(model.parameters(), lr=0.1)\n","\n","\n","        save_path = os.path.join(os.getcwd(), 'models', self.name)\n","        if not os.path.exists(save_path):\n","            os.makedirs(save_path)\n","            \n","        self.save_path = save_path\n","\n","    def train(self, epochs, save=True):\n","        '''\n","        @epochs: number of epochs to train\n","        @save: whether or not to save the checkpoints\n","        '''\n","\n","        # best_val_accuracy = - math.inf\n","        gradient_steps = 0\n","        for epoch in tqdm(range(epochs)):\n","\n","            self.model.train()\n","\n","            n_step = 512\n","            \n","            # TODO Iterate over the training dataloader (see how it is done for validation below) and make sure\n","            # to call the optim.zero_grad(), loss.backward() and optim.step()\n","            for idx, data in enumerate(self.trainset):\n","                inputs, labels = data\n","\n","                inputs = inputs.to('cuda' if self.use_cuda else 'cpu')\n","                labels = labels.to('cuda' if self.use_cuda else 'cpu')\n","\n","                outputs = self.model(inputs)\n","\n","                loss = self.criterion(outputs, labels)\n","\n","                optimizer = self.optim\n","\n","                lr = self.init_lr / math.sqrt(1.0 + math.floor(gradient_steps/n_step))\n","                gradient_steps += 1\n","                optimizer.param_groups[0]['lr'] = lr\n","                # print(gradient_steps, optimizer.param_groups[0]['lr'])\n","\n","                optimizer.zero_grad()\n","                loss.backward()\n","                optimizer.step()\n","\n","        if save:\n","            #  Make sure that your saving pipeline is working well. \n","            # Is os library working on your file system? \n","            # Is your model being saved and reloaded fine? \n","            # When you do the kernel viz, activation maps, \n","            # and GradCAM you must be using the model you have saved before.\n","            \n","            # torch.save(self.model.state_dict(), os.path.join(self.save_path, f'epoch_{epoch}.pt'))\n","            \n","            torch.save(self.model.state_dict(), os.path.join(self.save_path, 'best.pt'))\n","                  \n","\n","        # print('Done training!')                       \n","\n","    def evaluate_train(self):\n","        \n","        try:\n","            assert os.path.exists(os.path.join(self.save_path, 'best.pt'))\n","            \n","        except:\n","            print('It appears you are testing the model without training. Please train first')\n","            return\n","        \n","        self.model.load_state_dict(torch.load(os.path.join(self.save_path, 'best.pt')))\n","        self.model.eval()\n","\n","        #total = len(self.dataloaders['test'])\n","        \n","        correct = 0.0\n","        total = 0.0\n","        for idx, data in enumerate(self.trainset):\n","            \n","                inputs, labels = data\n","\n","                inputs = inputs.to('cuda' if self.use_cuda else 'cpu')\n","                labels = labels.to('cuda' if self.use_cuda else 'cpu')\n","                \n","                outputs = self.model(inputs)\n","                _, predicted = torch.max(outputs, 1)\n","                \n","                total += labels.shape[0]\n","                correct += (predicted == labels).sum().item()\n","                \n","        # print(f'Training Accuracy: {100 * correct/total}%')\n","        return 1 - correct/total\n","\n","    def evaluate_test(self):\n","        \n","        try:\n","            assert os.path.exists(os.path.join(self.save_path, 'best.pt'))\n","            \n","        except:\n","            print('It appears you are testing the model without training. Please train first')\n","            return\n","        \n","        self.model.load_state_dict(torch.load(os.path.join(self.save_path, 'best.pt')))\n","        self.model.eval()\n","\n","        #total = len(self.dataloaders['test'])\n","        \n","        correct = 0.0\n","        total = 0.0\n","        for idx, data in enumerate(self.testset):\n","            \n","                inputs, labels = data\n","\n","                inputs = inputs.to('cuda' if self.use_cuda else 'cpu')\n","                labels = labels.to('cuda' if self.use_cuda else 'cpu')\n","                \n","                outputs = self.model(inputs)\n","                _, predicted = torch.max(outputs, 1)\n","                total += labels.shape[0]\n","                correct += (predicted == labels).sum().item()\n","                \n","        # print(f'Testing Accuracy: {100 * correct/total}%')\n","        return 1 - correct/total"],"metadata":{"id":"w8ROaooKplo5","executionInfo":{"status":"ok","timestamp":1651366689387,"user_tz":240,"elapsed":2,"user":{"displayName":"YUNHANG LIN","userId":"04558973430853141896"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["batch_size = 128\n","num_workers = 2\n","# epochs = 500_000 // (50_000 // 128) # total number desirec SGD steps / number batches per epoch\n","epochs = 1000\n","label_noise_int = 20\n","label_noise = label_noise_int / 100\n","trainset, testset = load_data(\"cifar10\", label_noise)\n","\n","trainset, _ = random_split(trainset, [10000, len(trainset)-10000])\n","testset, _ = random_split(testset, [2000, len(testset)-2000])\n","\n","trainset = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n","testset = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=num_workers)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xn_jiYkWrZUG","executionInfo":{"status":"ok","timestamp":1651366702009,"user_tz":240,"elapsed":2454,"user":{"displayName":"YUNHANG LIN","userId":"04558973430853141896"}},"outputId":"0d89482e-a742-4391-dc03-f86f82a66039"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n","Files already downloaded and verified\n"]}]},{"cell_type":"code","source":["train_result = []\n","test_result = []\n","convnet_widths = [(i + 1) for i in range(16)] + [16 + 4*(i + 1) for i in range(12)] \n","for width in convnet_widths:\n","  experiment_name = f'CNN_{width}_{label_noise_int}%noise'  #Provide name to model experiment\n","  model_name = f'CNN_{width}_{label_noise_int}%noise' \n","\n","  model = CNN(width)\n","\n","  classifier = Classifier(experiment_name, model, trainset, testset, use_cuda=True)\n","\n","  classifier.train(epochs=epochs)\n","  train_error = classifier.evaluate_train()\n","  test_error = classifier.evaluate_test()\n","  print(f'width = {width}, train_error = {train_error}, test_error = {test_error}')\n","  train_result.append(train_error)\n","  test_result.append(test_error)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9TIwitVJx4jf","outputId":"f84b888c-5abb-440c-d5d6-a6f757e56b72"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 1000/1000 [21:14<00:00,  1.27s/it]\n"]},{"output_type":"stream","name":"stdout","text":["width = 1, train_error = 0.6472, test_error = 0.629\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1000/1000 [21:29<00:00,  1.29s/it]\n"]},{"output_type":"stream","name":"stdout","text":["width = 2, train_error = 0.5396000000000001, test_error = 0.5409999999999999\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1000/1000 [22:00<00:00,  1.32s/it]\n"]},{"output_type":"stream","name":"stdout","text":["width = 3, train_error = 0.5363, test_error = 0.5375\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1000/1000 [22:14<00:00,  1.33s/it]\n"]},{"output_type":"stream","name":"stdout","text":["width = 4, train_error = 0.5154000000000001, test_error = 0.5075000000000001\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1000/1000 [22:36<00:00,  1.36s/it]\n"]},{"output_type":"stream","name":"stdout","text":["width = 5, train_error = 0.6345000000000001, test_error = 0.641\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1000/1000 [22:32<00:00,  1.35s/it]\n"]},{"output_type":"stream","name":"stdout","text":["width = 6, train_error = 0.659, test_error = 0.649\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1000/1000 [22:55<00:00,  1.38s/it]\n"]},{"output_type":"stream","name":"stdout","text":["width = 7, train_error = 0.486, test_error = 0.488\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1000/1000 [23:12<00:00,  1.39s/it]\n"]},{"output_type":"stream","name":"stdout","text":["width = 8, train_error = 0.6159, test_error = 0.5915\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1000/1000 [23:31<00:00,  1.41s/it]\n"]},{"output_type":"stream","name":"stdout","text":["width = 9, train_error = 0.6253, test_error = 0.622\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1000/1000 [26:35<00:00,  1.60s/it]\n"]},{"output_type":"stream","name":"stdout","text":["width = 10, train_error = 0.6786, test_error = 0.6759999999999999\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1000/1000 [27:00<00:00,  1.62s/it]\n"]},{"output_type":"stream","name":"stdout","text":["width = 11, train_error = 0.6694, test_error = 0.6699999999999999\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1000/1000 [25:38<00:00,  1.54s/it]\n"]},{"output_type":"stream","name":"stdout","text":["width = 12, train_error = 0.6493, test_error = 0.6355\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1000/1000 [26:19<00:00,  1.58s/it]\n"]},{"output_type":"stream","name":"stdout","text":["width = 13, train_error = 0.6402, test_error = 0.642\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1000/1000 [27:08<00:00,  1.63s/it]\n"]},{"output_type":"stream","name":"stdout","text":["width = 14, train_error = 0.7602, test_error = 0.7444999999999999\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1000/1000 [28:01<00:00,  1.68s/it]\n"]},{"output_type":"stream","name":"stdout","text":["width = 15, train_error = 0.7123999999999999, test_error = 0.6950000000000001\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1000/1000 [28:33<00:00,  1.71s/it]\n"]},{"output_type":"stream","name":"stdout","text":["width = 16, train_error = 0.5432, test_error = 0.5485\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1000/1000 [41:22<00:00,  2.48s/it]\n"]},{"output_type":"stream","name":"stdout","text":["width = 20, train_error = 0.8318, test_error = 0.8345\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1000/1000 [36:50<00:00,  2.21s/it]\n"]},{"output_type":"stream","name":"stdout","text":["width = 24, train_error = 0.5562, test_error = 0.538\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1000/1000 [40:26<00:00,  2.43s/it]\n"]},{"output_type":"stream","name":"stdout","text":["width = 28, train_error = 0.7735, test_error = 0.7555000000000001\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1000/1000 [38:43<00:00,  2.32s/it]\n"]},{"output_type":"stream","name":"stdout","text":["width = 32, train_error = 0.6675, test_error = 0.659\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1000/1000 [43:28<00:00,  2.61s/it]\n"]},{"output_type":"stream","name":"stdout","text":["width = 36, train_error = 0.8012, test_error = 0.81\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1000/1000 [51:29<00:00,  3.09s/it]\n"]},{"output_type":"stream","name":"stdout","text":["width = 40, train_error = 0.604, test_error = 0.585\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1000/1000 [55:19<00:00,  3.32s/it]\n"]},{"output_type":"stream","name":"stdout","text":["width = 44, train_error = 0.5666, test_error = 0.5685\n"]},{"output_type":"stream","name":"stderr","text":[" 78%|███████▊  | 785/1000 [46:41<12:47,  3.57s/it]"]}]},{"cell_type":"code","source":["train_result = []\n","test_result = []\n","for width in range(17, 31):\n","  experiment_name = f'CNN_{width}_{label_noise_int}%noise'  #Provide name to model experiment\n","  model_name = f'CNN_{width}_{label_noise_int}%noise' \n","\n","  model = CNN(width)\n","\n","  classifier = Classifier(experiment_name, model, trainset, testset, use_cuda=True)\n","\n","  classifier.train(epochs=epochs)\n","  train_error = classifier.evaluate_train()\n","  test_error = classifier.evaluate_test()\n","  print(f'width = {width}, train_error = {train_error}, test_error = {test_error}')\n","  train_result.append(train_error)\n","  test_result.append(test_error)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mgeygsT3vfKf","outputId":"5361321b-283f-41d9-ff51-5bc44ca183c0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 1282/1282 [21:50<00:00,  1.02s/it]\n"]},{"output_type":"stream","name":"stdout","text":["width = 17, train_error = 0.0, test_error = 0.4195\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1282/1282 [22:58<00:00,  1.08s/it]\n"]},{"output_type":"stream","name":"stdout","text":["width = 18, train_error = 0.0, test_error = 0.42800000000000005\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1282/1282 [24:32<00:00,  1.15s/it]\n"]},{"output_type":"stream","name":"stdout","text":["width = 19, train_error = 0.0, test_error = 0.38449999999999995\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1282/1282 [26:07<00:00,  1.22s/it]\n"]},{"output_type":"stream","name":"stdout","text":["width = 20, train_error = 0.0, test_error = 0.40900000000000003\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1282/1282 [27:26<00:00,  1.28s/it]\n"]},{"output_type":"stream","name":"stdout","text":["width = 21, train_error = 0.0, test_error = 0.394\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1282/1282 [28:05<00:00,  1.31s/it]\n"]},{"output_type":"stream","name":"stdout","text":["width = 22, train_error = 0.0, test_error = 0.394\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1282/1282 [29:21<00:00,  1.37s/it]\n"]},{"output_type":"stream","name":"stdout","text":["width = 23, train_error = 0.0, test_error = 0.372\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1282/1282 [29:54<00:00,  1.40s/it]\n"]},{"output_type":"stream","name":"stdout","text":["width = 24, train_error = 0.0, test_error = 0.392\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1282/1282 [35:06<00:00,  1.64s/it]\n"]},{"output_type":"stream","name":"stdout","text":["width = 25, train_error = 0.0, test_error = 0.389\n"]},{"output_type":"stream","name":"stderr","text":[" 35%|███▌      | 455/1282 [12:46<23:21,  1.69s/it]"]}]},{"cell_type":"code","source":["train_result = []\n","test_result = []\n","for width in range(1, 31):\n","  experiment_name = f'CNN_{width}_{label_noise_int}%noise'  #Provide name to model experiment\n","  model_name = f'CNN_{width}_{label_noise_int}%noise' \n","\n","  model = CNN(width)\n","\n","  classifier = Classifier(experiment_name, model, trainset, testset, use_cuda=True)\n","\n","  classifier.train(epochs=epochs)\n","  train_error = classifier.evaluate_train()\n","  test_error = classifier.evaluate_test()\n","  print(f'width = {width}, train_error = {train_error}, test_error = {test_error}')\n","  train_result.append(train_error)\n","  test_result.append(test_error)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xfgji08PpAzS","outputId":"e53de6f3-f1d9-4ff0-fe89-2c78149e14dd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 1282/1282 [28:10<00:00,  1.32s/it]\n"]},{"output_type":"stream","name":"stdout","text":["width = 1, train_error = 0.632, test_error = 0.6295\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1282/1282 [27:59<00:00,  1.31s/it]\n"]},{"output_type":"stream","name":"stdout","text":["width = 2, train_error = 0.48660000000000003, test_error = 0.5569999999999999\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1282/1282 [28:03<00:00,  1.31s/it]\n"]},{"output_type":"stream","name":"stdout","text":["width = 3, train_error = 0.35329999999999995, test_error = 0.589\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1282/1282 [27:41<00:00,  1.30s/it]\n"]},{"output_type":"stream","name":"stdout","text":["width = 4, train_error = 0.22499999999999998, test_error = 0.609\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1282/1282 [28:00<00:00,  1.31s/it]\n"]},{"output_type":"stream","name":"stdout","text":["width = 5, train_error = 0.08540000000000003, test_error = 0.602\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1282/1282 [30:13<00:00,  1.41s/it]\n"]},{"output_type":"stream","name":"stdout","text":["width = 6, train_error = 0.17769999999999997, test_error = 0.579\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1282/1282 [30:42<00:00,  1.44s/it]\n"]},{"output_type":"stream","name":"stdout","text":["width = 7, train_error = 0.0, test_error = 0.567\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1282/1282 [29:53<00:00,  1.40s/it]\n"]},{"output_type":"stream","name":"stdout","text":["width = 8, train_error = 9.999999999998899e-05, test_error = 0.5389999999999999\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1282/1282 [30:04<00:00,  1.41s/it]\n"]},{"output_type":"stream","name":"stdout","text":["width = 9, train_error = 0.0, test_error = 0.5075000000000001\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1282/1282 [31:52<00:00,  1.49s/it]\n"]},{"output_type":"stream","name":"stdout","text":["width = 10, train_error = 0.0, test_error = 0.502\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1282/1282 [31:52<00:00,  1.49s/it]\n"]},{"output_type":"stream","name":"stdout","text":["width = 11, train_error = 0.0, test_error = 0.487\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1282/1282 [32:35<00:00,  1.53s/it]\n"]},{"output_type":"stream","name":"stdout","text":["width = 12, train_error = 0.0, test_error = 0.493\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1282/1282 [32:45<00:00,  1.53s/it]\n"]},{"output_type":"stream","name":"stdout","text":["width = 13, train_error = 0.0, test_error = 0.4585\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1282/1282 [34:30<00:00,  1.62s/it]\n"]},{"output_type":"stream","name":"stdout","text":["width = 14, train_error = 0.0, test_error = 0.44199999999999995\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1282/1282 [34:38<00:00,  1.62s/it]\n"]},{"output_type":"stream","name":"stdout","text":["width = 15, train_error = 9.999999999998899e-05, test_error = 0.45599999999999996\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1282/1282 [34:27<00:00,  1.61s/it]\n"]},{"output_type":"stream","name":"stdout","text":["width = 16, train_error = 0.0, test_error = 0.42500000000000004\n"]},{"output_type":"stream","name":"stderr","text":[" 92%|█████████▏| 1180/1282 [32:19<02:47,  1.65s/it]"]}]}]}